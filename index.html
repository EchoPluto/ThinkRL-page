<!DOCTYPE html>
<html>





<head>

  <!-- <link href="css/two_style.css" rel="stylesheet" /> -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="ThinkRL">
  <meta property="og:title" content="ThinkRL" />
  <meta property="og:description"
    content="ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/video_t1.png" />
  <meta property="og:image:width" content="2412" />
  <meta property="og:image:height" content="1394" />



  <title>ThinkRL</title>
  <link rel="icon" href="static/images/logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</head>

<body>

  <section class="hero banner" style="display: flex; align-items: center; justify-content: center; text-align: center;">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <i class="pixart-alpha-icon"></i>
                    <h1 class="title is-1 publication-title" style="display: flex; align-items: center; justify-content: center; text-align: center;">
                        <span>
                            <span style="color: #B771E5; font-weight: bold; font-size: 40px;">ThinkRL-Edit:</span>
                            <span style="color: white; font-weight: bold; font-size: 40px;">Thinking in Reinforcement Learning for</span><br>
                            <span style="color: white; font-weight: bold; font-size: 40px;">Reasoning-Centric Image Editing</span>
                            <!-- <span style="color: white; font-weight: bold; font-size: 52px;">with Precise Motion Control</span> -->
                        </span>
                    </h1>
                    <div class="is-size-5 publication-authors" style="text-align: center;">
                      <span class="author-block">
                          <a href="https://echopluto.github.io/HomePage/" target="_blank" style="color: rgb(170,170,170) !important; text-decoration: underline;">Hengjia Li</a><sup>1,2*</sup>,&thinsp;
                      </span>
                      <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=v_D9J7kAAAAJ&hl=zh-CN" target="_blank" style="color: rgb(170,170,170) !important; text-decoration: underline;">Liming Jiang</a><sup>2†</sup>,&thinsp;
                      </span>
                      <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=0TIYjPAAAAAJ&hl=en" target="_blank" style="color: rgb(170,170,170) !important; text-decoration: underline;">Qing Yan</a><sup>2</sup>,&thinsp;
                      </span>
                      <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=IUj5R3EAAAAJ&hl=en" target="_blank" style="color: rgb(170,170,170) !important; text-decoration: underline;">Yizhi Song</a><sup>2</sup>, &thinsp;
                      </span>
                      <br>
                      <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=VeTCSyEAAAAJ&hl=en" target="_blank" style="color: rgb(170,170,170) !important; text-decoration: underline;">Hao Kang</a><sup>2</sup>&thinsp;
                      </span>
                      <span class="author-block">
                        <a href="https://scholar.google.com/citations?hl=en&user=-H18WY8AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" style="color: rgb(170,170,170) !important; text-decoration: underline;">Zichuan Liu</a><sup>2</sup>&thinsp;
                      </span>
                      <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=mFC0wp8AAAAJ&hl=en" target="_blank" style="color: rgb(170,170,170) !important; text-decoration: underline;">Xin Lu</a><sup>2</sup>&thinsp;
                      </span>
                      <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=AqDe35sAAAAJ&hl=zh-CN" target="_blank" style="color: rgb(170,170,170) !important; text-decoration: underline;">Boxi Wu</a><sup>1§</sup>&thinsp;
                      </span>
                      <span class="author-block">
                          <a href="http://www.cad.zju.edu.cn/home/dengcai/" target="_blank" style="color: rgb(170,170,170) !important; text-decoration: underline;">Deng Cai</a><sup>1</sup>&thinsp;
                      </span>
                  </div>
                  <div class="is-size-5 publication-authors" style="text-align: center; opacity: 0.7;">
                      <span class="author-block"><sup>1</sup>Zhejiang University&thinsp; &thinsp;</span>
                      <span class="author-block"><sup>2</sup>Intelligent Creation, ByteDance&thinsp; &thinsp;</span>
                  </div>
                  <div class="is-size-5 publication-authors" style="text-align: center; opacity: 0.5;">
                    <span class="author-block"><sup>*</sup>Work done during internship at ByteDance &thinsp; &thinsp;</span>
                    <span class="author-block"><sup>†</sup>Project Lead &thinsp; &thinsp;</span>
                    <span class="author-block"><sup>§</sup>Corresponding Author&thinsp; &thinsp;</span>
                </div>
                  <div class="column has-text-centered">
                      <div class="publication-links">
                          <span class="link-block">
                              <a href="" target="_blank" class="external-link button is-normal is-rounded is-white">
                                  <span class="icon">
                                      <i class="fas fa-file-pdf"></i>
                                  </span>
                                  <span>Paper</span>
                              </a>
                          </span>
                          <span class="link-block">
                              <a href="" target="_blank" class="external-link button is-normal is-rounded is-white">
                                  <span class="icon">
                                      <i class="fab fa-github"></i>
                                  </span>
                                  <span>Code</span>
                              </a>
                          </span>
                      </div>
                  </div>
                </div>
            </div>
        </div>
    </div>
  </section>



  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <!-- <div class="hero-body"> -->
        <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/images/teaser.png"
        type="video/mp4">
      </video> -->
      <br>
        <img src="./static/images/teaser.png"
          alt="MY ALT TEXT" />
          <h2 class="content has-text-justified">
            Although unified multimodal generative models such as Qwen-Edit have substantially improved editing quality, their underlying reasoning remains underexplored, especially for reasoning-centric editing. In contrast, our method delivers accurate edits with deep reasoning, achieving strong consistency and high perceptual quality across diverse reasoning-driven editing scenarios.
      </div>
      <!-- </div> -->
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <!-- <div class="hero-body"> -->
        <video poster="" id="tree" controls muted loop height="100%">
          <source src="./static/images/demo.mp4"
            type="video/mp4">
        </video>
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <!-- <div class="columns is-centered has-text-centered"> -->
        <!-- <div class="column is-four-fifths"> -->
        <h2 class="title is-3">Abstract</h2>
        <!-- <h2 class="title is-3">Abstract</h2> -->
        <div class="content has-text-justified">
          <p>
            Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards.
            In this work, we propose ThinkRL-Edit, a reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)–based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to a visual outcome. 
            To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with a binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits.        <!-- </div> -->
        <!-- </div> -->
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Image carousel -->
  <section class="hero is-small">
    <!-- <section class="hero is-light"> -->
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">Motivation</h2>
        <div class="item">
          <!-- Your image here -->
          <img src="./static/images/motivation.png"
            alt="MY ALT TEXT" />
          <h2 class="content has-text-justified">
            Prior RL methods for visual generation focus on exploration within the stochastic space of generation, improving synthesis quality but offering limited reasoning capability. To address this issue, we decouple and optimize the understanding and generation modules to preserve high-fidelity synthesis while enabling exploration of optimal trajectories in the reasoning space. Besides, we introduce CoT-based sampling and optimization to further expand stochastic exploration over reasoning pathways.    </div>
    </div>
    </div>
  </section>
  <!-- End image carousel -->


  <!-- Image carousel -->
  <section class="hero is-small">
    <!-- <section class="hero is-light"> -->
    <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
        <h2 class="title is-3">Overall Framework of ThinkRL-Edit</h2>
        <div class="item">
          <!-- Your image here -->
          <img src="./static/images/method.png"
            alt="MY ALT TEXT" />
          <h2 class="content has-text-justified">
            During sampling, we perform Chain-of-Thought reasoning with explicit planning and reflection to enlarge stochasticity in the reasoning space. For rewards, a fine-grained, sample-specific checklist guides the VLM to produce accurate and stable reasoning scores. In grouping, we construct an unbiased preference chain across candidates to select training samples and compute advantages $A$. Finally, policy updates apply a unified editing reward while decoupling updates to the reasoning, understanding, and generation modules, enhancing reasoning capability without sacrificing synthesis quality.      </div>
    </div>
    </div>
    </div>
  </section>
  <!-- End image carousel -->

    <!-- Image carousel -->
    <section class="hero is-small">
      <!-- <section class="hero is-light"> -->
      <div class="hero-body">
        <div class="container is-max-desktop is-centered has-text-centered">
          <h2 class="title is-3">Qualitative Comparison with Baselines</h2>
          <div class="item">
            <!-- Your image here -->
            <img src="./static/images/comp.png"
              alt="MY ALT TEXT" />
            <h2 class="content has-text-justified">
              We conduct the comparison across diverse reasoning-centric editing tasks. As observed, our method achieves precise instruction following with strong consistency and high quality, which significantly surpasses previous methods. Blue text denotes the instruction, and green text indicates the desired editing outcome.      </div>
      </div>
      </div>
    </section>
    <!-- End image carousel -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8 text-center">
          <div class="content text-center">

            <p>
              <!-- <font color='red'>
                The images used in the demo videos are for academic purposes only and are not for commercial use. If you
                feel that the images we use infringe your rights, please contact us and we will delete them immediately
                and express our sincere apologies. Contact email: .</font> -->
              The source code of this webpage is borrowed from the <a href="https://nerfies.github.io"
                target="_blank">Nerfies</a> project webpage.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <script>
    bulmaCarousel.attach('#results-carousel21', {
      slidesToScroll: 1,
      slidesToShow: 2,
      infinite: true,
      autoplay: true,
      pauseOnHover: true,
      autoplaySpeed: 6000,
    });
    bulmaCarousel.attach('#results-carousel11', {
      slidesToScroll: 1,
      slidesToShow: 1,
      infinite: true,
      autoplay: true,
      pagination: true,
      pauseOnHover: true,
      autoplaySpeed: 6000,
    });
    bulmaCarousel.attach('#results-carousel33', {
      slidesToScroll: 1,
      slidesToShow: 2,
      infinite: true,
      autoplay: false,
      autoplaySpeed: 6000,
    });
  </script>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
